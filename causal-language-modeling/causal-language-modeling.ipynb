{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fine tuning Hugging Face for causal language modeling\n",
    "[![Open in Layer](https://app.layer.ai/assets/badge.svg)](https://app.layer.ai/layer/causal-language-modeling) [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/layerai/examples/blob/main/causal-language-modeling/causal-language-modeling.ipynb) [![Layer Examples Github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/layerai/examples/tree/main/causal-language-modeling)\n",
    "\n",
    "In this project we fine tune a Hugging Face model for text generation on the wikitext dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install layer --upgrade -qqq"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import layer\n",
    "layer.login()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-05T07:54:33.466929Z",
     "iopub.execute_input": "2022-04-05T07:54:33.467209Z",
     "iopub.status.idle": "2022-04-05T07:54:33.474008Z",
     "shell.execute_reply.started": "2022-04-05T07:54:33.467177Z",
     "shell.execute_reply": "2022-04-05T07:54:33.473158Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "layer.init(\"causal-language-modeling\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-05T07:54:34.745443Z",
     "iopub.execute_input": "2022-04-05T07:54:34.745697Z",
     "iopub.status.idle": "2022-04-05T07:54:35.341326Z",
     "shell.execute_reply.started": "2022-04-05T07:54:34.745669Z",
     "shell.execute_reply": "2022-04-05T07:54:35.340606Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from layer.decorators import model,pip_requirements,fabric"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-05T07:54:39.661460Z",
     "iopub.execute_input": "2022-04-05T07:54:39.661827Z",
     "iopub.status.idle": "2022-04-05T07:54:39.675100Z",
     "shell.execute_reply.started": "2022-04-05T07:54:39.661789Z",
     "shell.execute_reply": "2022-04-05T07:54:39.674361Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@pip_requirements(packages=[\"transformers\",\"sentencepiece\"])\n",
    "@fabric(\"f-medium\")\n",
    "@model(name=\"tokenizer\")\n",
    "def download_tokenizer():\n",
    "    from transformers import GPT2Tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-05T07:58:27.686742Z",
     "iopub.execute_input": "2022-04-05T07:58:27.687291Z",
     "iopub.status.idle": "2022-04-05T07:58:27.692539Z",
     "shell.execute_reply.started": "2022-04-05T07:58:27.687251Z",
     "shell.execute_reply": "2022-04-05T07:58:27.691575Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "download_tokenizer()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-05T07:58:28.221946Z",
     "iopub.execute_input": "2022-04-05T07:58:28.222432Z",
     "iopub.status.idle": "2022-04-05T07:58:38.283874Z",
     "shell.execute_reply.started": "2022-04-05T07:58:28.222396Z",
     "shell.execute_reply": "2022-04-05T07:58:38.283147Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def tokenize_function(examples):\n",
    "    from transformers import GPT2Tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    return tokenizer(examples[\"text\"])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-05T07:58:38.290033Z",
     "iopub.execute_input": "2022-04-05T07:58:38.293852Z",
     "iopub.status.idle": "2022-04-05T07:58:38.300528Z",
     "shell.execute_reply.started": "2022-04-05T07:58:38.293811Z",
     "shell.execute_reply": "2022-04-05T07:58:38.298994Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    # block_size = tokenizer.model_max_length\n",
    "    block_size = 128\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-05T07:58:38.305874Z",
     "iopub.execute_input": "2022-04-05T07:58:38.307499Z",
     "iopub.status.idle": "2022-04-05T07:58:38.323558Z",
     "shell.execute_reply.started": "2022-04-05T07:58:38.307460Z",
     "shell.execute_reply": "2022-04-05T07:58:38.322481Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@pip_requirements(packages=[\"transformers\",\"datasets\",\"sentencepiece\"])\n",
    "@fabric(\"f-gpu-small\")\n",
    "@model(\"gpt2-clm\")\n",
    "def train():\n",
    "        from datasets import load_dataset\n",
    "        from transformers import AutoTokenizer\n",
    "        from transformers import AutoConfig, TFAutoModelForCausalLM\n",
    "        from transformers import AdamWeightDecay\n",
    "        from transformers import DefaultDataCollator\n",
    "        import math\n",
    "        \n",
    "        datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "        model_checkpoint = \"gpt2\"\n",
    "        tokenizer_checkpoint = \"gpt2\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)\n",
    "        tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
    "        lm_datasets = tokenized_datasets.map(\n",
    "            group_texts,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            num_proc=4,)\n",
    "\n",
    "        config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "        model = TFAutoModelForCausalLM.from_config(config)\n",
    "        learning_rate = 2e-5\n",
    "        weight_decay = 0.01\n",
    "\n",
    "        optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "        model.compile(optimizer=optimizer)\n",
    "\n",
    "        data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "\n",
    "        train_set = lm_datasets[\"train\"].to_tf_dataset(\n",
    "            columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "            shuffle=True,\n",
    "            batch_size=16,\n",
    "            collate_fn=data_collator,)\n",
    "        \n",
    "        validation_set = lm_datasets[\"validation\"].to_tf_dataset(\n",
    "            columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "            shuffle=False,\n",
    "            batch_size=16,\n",
    "            collate_fn=data_collator,)\n",
    "\n",
    "        model.fit(train_set, validation_data=validation_set, epochs=2)\n",
    "        eval_loss = model.evaluate(validation_set)\n",
    "        print(f\"Perplexity: {math.exp(eval_loss):.2f}\")\n",
    "        return model"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-05T07:58:38.327095Z",
     "iopub.execute_input": "2022-04-05T07:58:38.328591Z",
     "iopub.status.idle": "2022-04-05T07:58:38.348219Z",
     "shell.execute_reply.started": "2022-04-05T07:58:38.328554Z",
     "shell.execute_reply": "2022-04-05T07:58:38.347606Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "layer.run([train])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-05T07:58:38.352486Z",
     "iopub.execute_input": "2022-04-05T07:58:38.354815Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "gpt2 = layer.get_model('gpt2-clm').get_train()\n",
    "tokenizer = layer.get_model('tokenizer').get_train()\n",
    "input_sequence = \"I love reading books\"\n",
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode(input_sequence, return_tensors='tf')\n",
    "output = gpt2.generate(input_ids)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(output[0], skip_special_tokens = True))"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
