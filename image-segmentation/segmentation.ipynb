{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZCM65CBt1CJ"
      },
      "source": [
        "# Image Segmentation with Layer\n",
        "\n",
        "[![Open in Layer](https://app.layer.ai/assets/badge.svg)](https://app.layer.ai/layer/image-segmentation) [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/layerai/examples/blob/main/image-segmentation/segmentation.ipynb) [![Layer Examples Github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/layerai/examples/tree/main/image-segmentation)\n",
        "\n",
        "In this project, we are going to focus on image segmentation with a modified [U-net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/). \n",
        "This project has been created out of the [Image Segmentation tutorial](https://www.tensorflow.org/tutorials/images/segmentation) of Tensorflow.\n",
        "\n",
        "\n",
        "## What is image segmentation?\n",
        "\n",
        "In an image classification task the network assigns a label (or class) to each input image. However, suppose you want to know the shape of that object, which pixel belongs to which object, etc. In this case you will want to assign a class to each pixel of the image. This task is known as segmentation. A segmentation model returns much more detailed information about the image. Image segmentation has many applications in medical imaging, self-driving cars and satellite imaging to name a few.\n",
        "\n",
        "This tutorial uses the [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/) ([Parkhi et al, 2012](https://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf)). The dataset consists of images of 37 pet breeds, with 200 images per breed (~100 each in the training and test splits). Each image includes the corresponding labels, and pixel-wise masks. The masks are class-labels for each pixel. Each pixel is given one of three categories:\n",
        "\n",
        "- Class 1: Pixel belonging to the pet.\n",
        "- Class 2: Pixel bordering the pet.\n",
        "- Class 3: None of the above/a surrounding pixel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Layer\n",
        "\n",
        "Let's start with installing Layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fhw6BAv8XaT9"
      },
      "outputs": [],
      "source": [
        "!pip install layer -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Login to Layer\n",
        "\n",
        "Let's login to Layer first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import layer\n",
        "layer.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Layer Project\n",
        "Now we are ready to init our project. Layer Project is basically an ML Repo hosted on Layer where you can store your datasets, models, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "layer.init(\"image-segmentation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWe0_rQM4JbC"
      },
      "source": [
        "## Download the Oxford-IIIT Pets dataset\n",
        "\n",
        "The dataset is [available from TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/oxford_iiit_pet). The segmentation masks are included in version 3+."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQX7R4bhZy5h"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40ITeStwDwZb"
      },
      "outputs": [],
      "source": [
        "def get_dataset():\n",
        "  import tensorflow_datasets as tfds  \n",
        "  return tfds.load('oxford_iiit_pet:3.*.*', with_info=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJcVdj_U4vzf"
      },
      "source": [
        " In addition, the image color values are normalized to the `[0,1]` range. Finally, as mentioned above the pixels in the segmentation mask are labeled either {1, 2, 3}. For the sake of convenience, subtract 1 from the segmentation mask, resulting in labels that are : {0, 1, 2}."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FD60EbcAQqov"
      },
      "outputs": [],
      "source": [
        "def normalize(input_image, input_mask):\n",
        "  input_image = tf.cast(input_image, tf.float32) / 255.0\n",
        "  input_mask -= 1\n",
        "  return input_image, input_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zf0S67hJRp3D"
      },
      "outputs": [],
      "source": [
        "def load_image(datapoint):\n",
        "  input_image = tf.image.resize(datapoint['image'], (128, 128))\n",
        "  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\n",
        "\n",
        "  input_image, input_mask = normalize(input_image, input_mask)\n",
        "\n",
        "  return input_image, input_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65-qHTjX5VZh"
      },
      "source": [
        "The dataset already contains the required training and test splits, so continue to use the same splits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9hGHyg8L3Y1"
      },
      "source": [
        "The following class performs a simple augmentation by randomly-flipping an image.\n",
        "Go to the [Image augmentation](data_augmentation.ipynb) tutorial to learn more.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUWdDJRTL0PP"
      },
      "outputs": [],
      "source": [
        "class Augment(tf.keras.layers.Layer):\n",
        "  def __init__(self, seed=42):\n",
        "    super().__init__()\n",
        "    # both use the same seed, so they'll make the same random changes.\n",
        "    self.augment_inputs = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
        "    self.augment_labels = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
        "  \n",
        "  def call(self, inputs, labels):\n",
        "    inputs = self.augment_inputs(inputs)\n",
        "    labels = self.augment_labels(labels)\n",
        "    return inputs, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTIbNIBdcgL3"
      },
      "source": [
        "Build the input pipeline, applying the Augmentation after batching the inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPscskQcNCx4"
      },
      "outputs": [],
      "source": [
        "def get_batches():\n",
        "  dataset, info = get_dataset()\n",
        "\n",
        "  TRAIN_LENGTH = info.splits['train'].num_examples\n",
        "  BATCH_SIZE = 64\n",
        "  BUFFER_SIZE = 1000\n",
        "  STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n",
        "\n",
        "  train_images = dataset['train'].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "  test_images = dataset['test'].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "  train_batches = (\n",
        "      train_images\n",
        "      .cache()\n",
        "      .shuffle(BUFFER_SIZE, seed=20)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .repeat()\n",
        "      .map(Augment())\n",
        "      .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "\n",
        "  test_batches = test_images.batch(BATCH_SIZE)\n",
        "  return train_batches, test_batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa3gMAE_9qNa"
      },
      "source": [
        "Visualize an image example and its corresponding mask from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3N2RPAAW9q4W"
      },
      "outputs": [],
      "source": [
        "def display(display_list):\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
        "\n",
        "  for i in range(len(display_list)):\n",
        "    plt.subplot(1, len(display_list), i+1)\n",
        "    plt.title(title[i])\n",
        "    plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAOe93FRMk3w"
      },
      "source": [
        "## Define the model\n",
        "The model being used here is a modified [U-Net](https://arxiv.org/abs/1505.04597). A U-Net consists of an encoder (downsampler) and decoder (upsampler). In-order to learn robust features and reduce the number of trainable parameters, you will use a pretrained model - MobileNetV2 - as the encoder. For the decoder, you will use the upsample block, which is already implemented in the [pix2pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py) example in the TensorFlow Examples repo. (Check out the [pix2pix: Image-to-image translation with a conditional GAN](../generative/pix2pix.ipynb) tutorial in a notebook.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4mQle3lthit"
      },
      "source": [
        "As mentioned, the encoder will be a pretrained MobileNetV2 model which is prepared and ready to use in `tf.keras.applications`. The encoder consists of specific outputs from intermediate layers in the model. Note that the encoder will not be trained during the training process.\n",
        "\n",
        "Following `upsample` function is from [Tensorflow Examples](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py) repo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def upsample(filters, size, norm_type='batchnorm', apply_dropout=False):\n",
        "  \"\"\"Upsamples an input.\n",
        "  Conv2DTranspose => Batchnorm => Dropout => Relu\n",
        "  Args:\n",
        "    filters: number of filters\n",
        "    size: filter size\n",
        "    norm_type: Normalization type; either 'batchnorm' or 'instancenorm'.\n",
        "    apply_dropout: If True, adds the dropout layer\n",
        "  Returns:\n",
        "    Upsample Sequential Model\n",
        "  \"\"\"\n",
        "\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "      tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                      padding='same',\n",
        "                                      kernel_initializer=initializer,\n",
        "                                      use_bias=False))\n",
        "\n",
        "  if norm_type.lower() == 'batchnorm':\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "  elif norm_type.lower() == 'instancenorm':\n",
        "    result.add(InstanceNormalization())\n",
        "\n",
        "  if apply_dropout:\n",
        "    result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "  result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45HByxpVtrPF"
      },
      "outputs": [],
      "source": [
        "def unet_model(output_channels:int):\n",
        "  base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\n",
        "\n",
        "  # Use the activations of these layers\n",
        "  layer_names = [\n",
        "      'block_1_expand_relu',   # 64x64\n",
        "      'block_3_expand_relu',   # 32x32\n",
        "      'block_6_expand_relu',   # 16x16\n",
        "      'block_13_expand_relu',  # 8x8\n",
        "      'block_16_project',      # 4x4\n",
        "  ]\n",
        "  base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
        "\n",
        "  # Create the feature extraction model\n",
        "  down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
        "\n",
        "  down_stack.trainable = False\n",
        "\n",
        "\n",
        "  up_stack = [\n",
        "    upsample(512, 3),  # 4x4 -> 8x8\n",
        "    upsample(256, 3),  # 8x8 -> 16x16\n",
        "    upsample(128, 3),  # 16x16 -> 32x32\n",
        "    upsample(64, 3),   # 32x32 -> 64x64\n",
        "  ]\n",
        "\n",
        "\n",
        "  inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = down_stack(inputs)\n",
        "  x = skips[-1]\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    concat = tf.keras.layers.Concatenate()\n",
        "    x = concat([x, skip])\n",
        "\n",
        "  # This is the last layer of the model\n",
        "  last = tf.keras.layers.Conv2DTranspose(\n",
        "      filters=output_channels, kernel_size=3, strides=2,\n",
        "      padding='same')  #64x64 -> 128x128\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRsjdZuEnZfA"
      },
      "source": [
        "Note that the number of filters on the last layer is set to the number of `output_channels`. This will be one output channel per class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0DGH_4T0VYn"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Now, all that is left to do is to compile and train the model. \n",
        "\n",
        "Since this is a multiclass classification problem, use the `tf.keras.losses.CategoricalCrossentropy` loss function with the `from_logits` argument set to `True`, since the labels are scalar integers instead of vectors of scores for each pixel of every class. \n",
        "\n",
        "When running inference, the label assigned to the pixel is the channel with the highest value. This is what the `create_mask` function is doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLNsrynNtx4d"
      },
      "outputs": [],
      "source": [
        "def show_predictions(model, ataset=None, num=1):\n",
        "  if dataset:\n",
        "    for image, mask in dataset.take(num):\n",
        "      pred_mask = model.predict(image)\n",
        "      display([image[0], mask[0], create_mask(pred_mask)])\n",
        "  else:\n",
        "    display([sample_image, sample_mask,\n",
        "             create_mask(model.predict(sample_image[tf.newaxis, ...]))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22AyVYWQdkgk"
      },
      "source": [
        "The callback defined below is used to observe how the model improves while it is training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHrHsqijdmL6"
      },
      "outputs": [],
      "source": [
        "class LayerCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    layer.log(logs, epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyLlkbFew9B6"
      },
      "outputs": [],
      "source": [
        "def array_to_img(arr):\n",
        "  return tf.keras.utils.array_to_img(arr).resize((512,512))\n",
        "\n",
        "def predict(model, img):\n",
        "  return create_mask(model.predict(img[tf.newaxis, ...]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwvIKLZPtxV_"
      },
      "outputs": [],
      "source": [
        "def create_mask(pred_mask):\n",
        "  pred_mask = tf.argmax(pred_mask, axis=-1)\n",
        "  pred_mask = pred_mask[..., tf.newaxis]\n",
        "  return pred_mask[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train your model with Layer\n",
        "\n",
        "You can train, register and share your model with Layer. Layer handles all the overhead, the pain, and the complexity of infrastructure. In this example we will train our mask prediction model on a remote GPU instance from our local notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441,
          "referenced_widgets": [
            "8b7aa02cfb1b46589aee1ea0d0833631",
            "1e7ce32e3add4bada02c0f8155891667"
          ]
        },
        "id": "StKDH_B9t4SD",
        "outputId": "d1ccc71f-2b99-4384-cca1-8a0c63c494e6"
      },
      "outputs": [],
      "source": [
        "from layer.decorators import model, fabric\n",
        "@model(\"mask_predictor\")\n",
        "@fabric(\"f-gpu-small\")\n",
        "def train():\n",
        "    train_batches, test_batches = get_batches()\n",
        "    parameters = {\"EPOCHS\": 20, \"STEPS_PER_EPOCH\" : 50, \n",
        "                \"OUTPUT_CLASSES\": 3, \"VALIDATION_STEPS\":10}\n",
        "    layer.log(parameters)\n",
        "\n",
        "    images, masks = next(test_batches.as_numpy_iterator())\n",
        "    layer.log({\"original_image\": array_to_img(images[0])})\n",
        "    layer.log({\"original_mask\": array_to_img(masks[0])})\n",
        "    mask_model = unet_model(output_channels=parameters[\"OUTPUT_CLASSES\"])\n",
        "    mask_model.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "    model_history = mask_model.fit(train_batches, epochs=parameters[\"EPOCHS\"],\n",
        "                            steps_per_epoch=parameters[\"STEPS_PER_EPOCH\"],\n",
        "                            validation_steps=parameters[\"VALIDATION_STEPS\"],\n",
        "                            validation_data=test_batches, callbacks=[LayerCallback()]\n",
        "                            )\n",
        "    layer.log({\"predicted_mask\": array_to_img(predict(mask_model, images[0]))})  \n",
        "\n",
        "    return mask_model\n",
        "\n",
        "layer.run([train])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unP3cnxo_N72"
      },
      "source": [
        "## Make predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BVXldSo-0mW"
      },
      "source": [
        "Now, let's make some predictions with fetching the model from Layer. We can fetch any version of our trained model and use it for predictions. Let's fetch the latest model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import layer\n",
        "my_model = layer.get_model('mask_predictor').get_train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_, test_batches = get_batches()\n",
        "images, masks = next(test_batches.as_numpy_iterator())\n",
        "display([images[0], masks[0], predict(my_model, images[0])])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Where to go from here?\n",
        "\n",
        "Now that you have ran your Layer Project, you can:\n",
        "\n",
        "- Join our [Slack Community ](https://bit.ly/layercommunityslack)\n",
        "- Visit [Layer Examples Repo](https://github.com/layerai/examples) for more examples\n",
        "- Browse [Trending Layer Projects](https://layer.ai) on our mainpage\n",
        "- Check out [Layer Documentation](https://docs.layer.ai) to learn more"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "segmentation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1e7ce32e3add4bada02c0f8155891667": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b7aa02cfb1b46589aee1ea0d0833631": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_1e7ce32e3add4bada02c0f8155891667",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅  segmentation         <span style=\"color: #34d399; text-decoration-color: #34d399\">━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #34d399; text-decoration-color: #34d399\">        done         </span> <span style=\"color: #000000; text-decoration-color: #000000\">[ </span><span style=\"color: #808000; text-decoration-color: #808000\">0:02:22</span><span style=\"color: #000000; text-decoration-color: #000000\"> ]</span> \n    <a href=\"https://app.layer.ai/layer/sandbox-mecevit/models/segmentation\"><span style=\"color: #a1a1a9; text-decoration-color: #a1a1a9; text-decoration: underline\">https://app.layer.ai/layer/sandbox-mecevit/models/segmentation</span></a>                \n</pre>\n",
                  "text/plain": "✅  segmentation         \u001b[38;2;52;211;153m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[38;2;52;211;153m        done         \u001b[0m \u001b[39m[ \u001b[0m\u001b[33m0:02:22\u001b[0m\u001b[39m ]\u001b[0m \n    \u001b]8;id=378942;https://app.layer.ai/layer/sandbox-mecevit/models/segmentation\u001b\\\u001b[4;38;2;161;161;169mhttps://app.layer.ai/layer/sandbox-mecevit/models/segmentation\u001b[0m\u001b]8;;\u001b\\                \n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
