{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ML PROJECT: Predict review score of an order based on its information\n",
    "\n",
    "[![Open in Layer](https://development.layer.co/assets/badge.svg)](https://app.layer.ai/layer/ecommerce_olist_order_review_score_prediction/) [![Layer Examples Github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/layerai/examples/blob/ecommerce/ecommerce-order-review-score)\n",
    "\n",
    "In this e-commerce example walkthrough, we will train a machine learning model to predict review scores of orders [a number between 1 and 5] based on some order and its items based features extracted from Brazilian e-commerce company OLIST's datasets. --> https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce"
   ],
   "metadata": {
    "id": "L-MuHlqu1T83"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Install Requirements"
   ],
   "metadata": {
    "id": "o1WpNQjG4efc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install layer --upgrade"
   ],
   "metadata": {
    "id": "1d-1SCbKuzbj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!rm -rf examples/ecommerce-order-review-score\n",
    "!git clone https://github.com/layerai/examples/"
   ],
   "metadata": {
    "id": "tIGFI7xQLZTH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Getting Started with Layer\n",
    "\n",
    "Layer is an MLOps platform which advances ML pipelines with remote computation and tracking."
   ],
   "metadata": {
    "id": "PQDufRgJ4mEu"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Login to Layer\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Let's login to Layer first."
   ],
   "metadata": {
    "id": "hoE3K66O4uDz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmjdLhCNwrS1"
   },
   "outputs": [],
   "source": [
    "#from layer.v2.assertions import greatexpectations, assert_true, assert_valid_values, assert_not_null, assert_unique\n",
    "from layer.decorators import dataset, model,resources, pip_requirements, assertions, fabric\n",
    "from layer.decorators.assertions import assert_unique, assert_valid_values\n",
    "from layer.client import Dataset, Model\n",
    "import layer\n",
    "\n",
    "layer.login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialize Layer Project\n",
    "Now we are ready to init our project. Layer Project is basically an ML Repo hosted on Layer where you can store your datasets, models, metrics"
   ],
   "metadata": {
    "id": "6G4FRoXO42xZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "## LAYER Project Initialize\n",
    "layer.init('ecommerce_olist_order_review_score_prediction',\n",
    "           fabric=\"f-small\",\n",
    "           pip_packages=[\"numpy\",\"sklearn\",\"pandas\"]\n",
    "           )"
   ],
   "metadata": {
    "id": "zZ1RI6b041jm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Your project is ready. Find your project here:\n",
    "\n",
    "https://app.layer.ai"
   ],
   "metadata": {
    "id": "4WCzwdLw56-1"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vE8xVJGKy6l"
   },
   "source": [
    "# **Data Transformation**\n",
    "\n",
    "We will create total of 9 Layer datasets in this project. Here is the list of those datasets and their little descriptions.\n",
    "\n",
    "From the *olist_orders_dataset.csv* file, we have created 3 datasets:\n",
    "\n",
    "*  **orders_raw_table:** This is basically identical the csv file. It just Layer Dataset definition of the same orders raw data.\n",
    "\n",
    "\n",
    "* **orders_clean_table:** This is the clean version of the orders data after applying some data transformation operations on the orders_raw_table. \n",
    "\n",
    "\n",
    "* **orders_based_features:** High level features extracted from the orders_clean_table.\n",
    "\n",
    "\n",
    "\n",
    "From the *olist_order_items_dataset.csv* file, we have created 3 datasets:\n",
    "* **items_raw_table:** This is basically identical the csv file. It just Layer Dataset definition of the same items raw data.\n",
    " \n",
    "\n",
    "* **items_clean_table:** This is the clean version of the items data after applying some data transformation operations on the items_raw_table. \n",
    "\n",
    "\n",
    "* **items_based_features:** High level features extracted from the items_clean_table.\n",
    "\n",
    "\n",
    "From the *olist_order_reviews_dataset.csv* file, we have created 2 datasets\n",
    "\n",
    "* **reviews_raw_table:** This is basically identical the csv file. It just Layer Dataset definition of the same reviews raw data.\n",
    "\n",
    "* **reviews_clean_table:** This dataset is created to extract target variable for the problem which is the review scores for the past orders. \n",
    "\n",
    "\n",
    "Finally, we created the **training_data** which merges the orders_based_features, items_based_features and reviews_clean_table. This dataset is used to train the model.\n",
    "\n",
    "Once you are done with building datasets, you can see them all here:\n",
    "https://app.layer.ai/layer/ecommerce_olist_order_review_score_prediction/datasets/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "quz2sjuoKu9Z"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "@resources(\"examples/ecommerce-order-review-score/olist_orders_dataset.csv\")\n",
    "@dataset(\"orders_raw_table\")\n",
    "def load_order_table():\n",
    "  orders_df = pd.read_csv(\"examples/ecommerce-order-review-score/olist_orders_dataset.csv\")\n",
    "\n",
    "  layer.log({\"Dataset Description\": \"Raw orders table by loading from a csv file\"})\n",
    "\n",
    "  return orders_df\n",
    "\n",
    "@dataset(\"orders_clean_table\",dependencies=[Dataset('orders_raw_table')])\n",
    "def clean_order_table():\n",
    "  # Load dataset\n",
    "  orders_df = layer.get_dataset(\"orders_raw_table\").to_pandas()\n",
    "\n",
    "  # Drop all the rows having at least 1 null value - Since there are just a few null values in the data, we could just drop all of them.\n",
    "  df = orders_df.dropna()\n",
    "\n",
    "  # We will do our analysis only on the delivered orders\n",
    "  df = df[df['order_status'] == 'delivered']\n",
    "\n",
    "  # Drop rows that don't meet the requirement: order_purchase_timestamp <= order_approved_at <= order_delivered_carrier_date <= order_delivered_customer_date\n",
    "  df = df[~((df['order_purchase_timestamp'] >= df['order_approved_at']) | (df['order_approved_at'] >= df['order_delivered_carrier_date']) | (df['order_delivered_carrier_date'] >= df['order_delivered_customer_date']))]\n",
    "\n",
    "  layer.log({\"Dataset Description\": \"Cleaned version of the orders table by dropping na rows, selecting only 'delivered' orders and doing sanity checks on timestamp columns.\"})\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "def bucketize_actual_delivery_vs_expectation (row):\n",
    "  if row['days_between_delivery_expectation'] <= 0 :\n",
    "    return -1\n",
    "  elif row['days_between_delivery_expectation'] <= 7 :\n",
    "    return 1\n",
    "  elif row['days_between_delivery_expectation'] <= 14 :\n",
    "    return 2\n",
    "  else:\n",
    "    return 3\n",
    "\n",
    "@dataset(\"orders_based_features\",dependencies=[Dataset('orders_clean_table')])\n",
    "def extract_features_order_table():\n",
    "  # Load dataset\n",
    "  df = layer.get_dataset(\"orders_clean_table\").to_pandas()\n",
    "\n",
    "  # Days between purchase and delivery dates\n",
    "  df['days_between_purhcase_and_delivery'] = (pd.to_datetime(df['order_delivered_customer_date']) - pd.to_datetime(df['order_purchase_timestamp'])).dt.days\n",
    "\n",
    "  # if the order was approved late or on time (0=on time, 1=late)\n",
    "  df['order_approved_late']=np.where((pd.to_datetime(df['order_approved_at']) - pd.to_datetime(df['order_purchase_timestamp'])).dt.days == 0, 0, 1)\n",
    "\n",
    "  # Actual delivery vs. Expected delivery: 1=Delivered before expected date, 2= Delivered one week later than expected date, 3= Delivered two weeks later than expected date, 4= Delivered more than two weeks later than expected date\n",
    "  df['days_between_delivery_expectation']=(pd.to_datetime(df['order_estimated_delivery_date']) - pd.to_datetime(df['order_delivered_customer_date'])).dt.days\n",
    "  df['actual_delivery_vs_expectation_bucket'] = df.apply (lambda row: bucketize_actual_delivery_vs_expectation(row), axis=1)\n",
    "\n",
    "  layer.log({\"Dataset Description\": \"Features extracted only from the orders table\"})\n",
    "  layer.log({\"days_between_purchase_and_delivery\":\"Days between delivery date and purchase date\",\n",
    "             \"order_approved_late\":\"0: Order payment is approved on the same day with purchase -- 1: Otherwise\",\n",
    "             \"actual_delivery_vs_expectation_bucket\":\"It represents the days between estimated delivery date and actual delivery date -- 1: Less than 7 days, 2: Less than 14 days more than 7 days, 3: More than 14 days, -1: Order delivered later than estimated date\",\n",
    "             \"order_delivered_carrier_date\":\"The date order delivered to carrier\"\n",
    "             })\n",
    "\n",
    "  df = df[['order_id','days_between_purhcase_and_delivery','order_approved_late','actual_delivery_vs_expectation_bucket','order_delivered_carrier_date']]\n",
    "  return df\n",
    "\n",
    "@resources(\"examples/ecommerce-order-review-score/olist_order_items_dataset.csv\")\n",
    "@dataset(\"items_raw_table\")\n",
    "def load_item_table():\n",
    "  # Load items table from csv file\n",
    "  items_df = pd.read_csv(\"examples/ecommerce-order-review-score/olist_order_items_dataset.csv\")\n",
    "\n",
    "  layer.log({\"Dataset Description\": \"Raw items table by loading from a csv file\"})\n",
    "\n",
    "  return items_df  \n",
    "\n",
    "\n",
    "@dataset(\"items_clean_table\",dependencies=[Dataset('items_raw_table')])\n",
    "def clean_items_table():\n",
    "  # Load dataset\n",
    "  items_df = layer.get_dataset(\"items_raw_table\").to_pandas()\n",
    "\n",
    "  # Select relevant columns and drop any na valued rows\n",
    "  df = items_df[['order_id','shipping_limit_date','price','freight_value']].dropna()\n",
    "\n",
    "  # Price and Freight Value must be non-negative\n",
    "  df = df[(items_df['price']>=0) & (items_df['freight_value']>=0)]\n",
    "\n",
    "  layer.log({\"Dataset Description\": \"Cleaned version of the raw items table by selecting some relevant columns out of it, dropping na rows and doing some sanity checks on 'price' and 'freight_value' columns\"})\n",
    "\n",
    "  return df\n",
    "\n",
    "@dataset(\"items_based_features\",dependencies=[Dataset('items_clean_table')])\n",
    "def extract_features_items_table():\n",
    "  # Load dataset\n",
    "  df = layer.get_dataset(\"items_clean_table\").to_pandas()\n",
    "  # Extract 3 features: total_order_price & total_order_freight\n",
    "  df1 = df.groupby('order_id').agg(total_order_price=('price', 'sum'), total_order_freight=('freight_value','sum'),max_shipping_limit_date=('shipping_limit_date','max')).reset_index()\n",
    "\n",
    "  # Extract 1 feature: is_multiItems_order -- If the order has multiple items or not (1 or 0)\n",
    "  df2 = df.groupby('order_id').agg(cnt=('price', 'count')).reset_index()\n",
    "  df2['is_multiItems_order'] = np.where(df2['cnt'] > 1, 1, 0)\n",
    "\n",
    "  df3 = df1.merge(df2, how=\"inner\", on='order_id')[['order_id','is_multiItems_order','total_order_price','total_order_freight','max_shipping_limit_date']]\n",
    "\n",
    "  layer.log({\"Dataset Description\": \"Features extracted only from the items table\"})\n",
    "  layer.log({\"total_order_price\":\"Total price paid for the order\",\n",
    "             \"total_order_freight\":\"Total price paid for freight transport\",\n",
    "             \"max_shipping_limit_date\":\"Maximum of expected shipping date if order has many items\",\n",
    "             \"is_multiItems_order\":\"If the order has multiple items or not. 0: Single item order -- 1: Multiple items order\"\n",
    "             })\n",
    "\n",
    "  return df3\n",
    "\n",
    "@resources(\"examples/ecommerce-order-review-score/olist_order_reviews_dataset.csv\")\n",
    "@dataset(\"reviews_raw_table\")\n",
    "def load_reviews_table():\n",
    "  # Load the reviews table from csv file\n",
    "  reviews_df = pd.read_csv(\"examples/ecommerce-order-review-score/olist_order_reviews_dataset.csv\")\n",
    "\n",
    "  layer.log({\"Dataset Description\": \"Raw reviews table by loading from a csv file\"})\n",
    "\n",
    "  return reviews_df\n",
    "\n",
    "@dataset(\"reviews_clean_table\",dependencies=[Dataset('reviews_raw_table')])\n",
    "def clean_reviews_table():\n",
    "  # Load dataset\n",
    "  reviews_df = layer.get_dataset(\"reviews_raw_table\").to_pandas()\n",
    "\n",
    "  # Drop extra reviews if an order has multiple order review scores\n",
    "  df = reviews_df.groupby('order_id', as_index= False).agg(review_score=('review_score', 'max'))\n",
    "\n",
    "  layer.log({\"Dataset Description\": \"This table is used to create labels (target variable) which is review scores.\"})\n",
    "  \n",
    "  return df\n",
    "\n",
    "@assert_unique([\"order_id\"])\n",
    "@assert_valid_values(\"review_score\", [1,2,3,4,5])\n",
    "@dataset(\"training_data\",dependencies=[Dataset('orders_based_features'),Dataset('items_based_features'),Dataset('reviews_clean_table')])\n",
    "def generate_training_data():\n",
    "  from functools import reduce\n",
    "  # Merge all clean datasets\n",
    "  orders_data = layer.get_dataset(\"orders_based_features\").to_pandas()\n",
    "  items_data = layer.get_dataset(\"items_based_features\").to_pandas()\n",
    "  reviews_data = layer.get_dataset(\"reviews_clean_table\").to_pandas()\n",
    "\n",
    "  data_frames = [orders_data, items_data, reviews_data]\n",
    "  df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['order_id'],how='inner'), data_frames)\n",
    "\n",
    "  # Create a new feature: seller_shipped_late\n",
    "  df_merged['days_between_delivered_carrier_and_shipping_limit'] = (pd.to_datetime(df_merged['order_delivered_carrier_date']) - pd.to_datetime(df_merged['max_shipping_limit_date'])).dt.days\n",
    "  df_merged['seller_shipped_late'] = np.where(df_merged['days_between_delivered_carrier_and_shipping_limit'] > 0 , 1 ,0) \n",
    "\n",
    "  # Select only relevant columns (features)\n",
    "  df_merged = df_merged.drop(columns=['order_delivered_carrier_date', 'max_shipping_limit_date','days_between_delivered_carrier_and_shipping_limit'])\n",
    "\n",
    "  layer.log({\"Dataset Description\": \"All features from the orders and items tables. Final training dataset.\"})\n",
    "  layer.log({\"order_id\":\"Unique id for the order\",\n",
    "             \"days_between_purchase_and_delivery\":\"Days between delivery date and purchase date\",\n",
    "             \"order_approved_late\":\"0: Order payment is approved on the same day with purchase -- 1: Otherwise\",\n",
    "             \"actual_delivery_vs_expectation_bucket\":\"Days between estimated delivery date and actual delivery date -- 1: Less than 7 days, 2: Less than 14 days more than 7 days, 3: More than 14 days, -1: Order delivered later than estimated date\",\n",
    "             \"total_order_price\":\"Total price paid for the order\",\n",
    "             \"total_order_freight\":\"Total price paid for freight transport\",\n",
    "             \"is_multiItems_order\":\"If the order has multiple items or not. 0: Single item order -- 1: Multiple items order\",\n",
    "             \"seller_shipped_late\":\"if seller shipped items later than promised date. 1: Late - 0: Before or on time\",\n",
    "             \"review_score\":\"Review score for the order between 1 and 5\",\n",
    "             })\n",
    "  \n",
    "  \n",
    "  return df_merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhHQZ4Olk2Mc"
   },
   "source": [
    "# **Model Functions Refined**\n",
    "\n",
    "We will be training a XGBRegressor from xgboost. We will fit the training dataset we have created. You can find all the model experiments and logged data here:\n",
    "\n",
    "Once you are done with training, you can see your model here:\n",
    "https://app.layer.ai/layer/ecommerce_olist_order_review_score_prediction/models/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tsv9TVtIk5YU"
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def data_split(data):\n",
    "  data = layer.get_dataset(\"training_data\").to_pandas()\n",
    "\n",
    "  training_data, testing_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "  # split data into train and test sets\n",
    "  X_train, X_valid, y_train, y_valid = train_test_split(training_data.drop(['review_score', 'order_id'], axis=1), training_data['review_score'], test_size=0.1, random_state=7)\n",
    "\n",
    "  data_pair = [(X_train, y_train), (X_valid, y_valid)]\n",
    "\n",
    "  return data_pair\n",
    "\n",
    "\n",
    "def model_overfit_check(X_train, y_train, eval_set):\n",
    "  param_dict = {\n",
    "    'colsample_bytree' : 1.0,\n",
    "    'learning_rate': 0.02,\n",
    "    'max_depth': 5,\n",
    "    'min_child_weight': 10,\n",
    "    'subsample' : 0.5\n",
    "  }\n",
    "\n",
    "  xgb_model = XGBRegressor(n_estimators=1000, objective='reg:squarederror', colsample_bytree = param_dict['colsample_bytree'], learning_rate=param_dict['learning_rate'], max_depth=param_dict['max_depth'], min_child_weight=param_dict['min_child_weight'], subsample=param_dict['subsample'])\n",
    "\n",
    "  xgb_model.fit(X_train, y_train, eval_metric='rmse', eval_set=eval_set, verbose=False)\n",
    "\n",
    "  # retrieve performance metrics\n",
    "  results = xgb_model.evals_result()\n",
    "  epochs = len(results['validation_0']['rmse'])\n",
    "  x_axis = range(0, epochs)\n",
    "\n",
    "  # plot rmse - train vs. test\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.plot(x_axis, results['validation_0']['rmse'], label='Train')\n",
    "  ax.plot(x_axis, results['validation_1']['rmse'], label='Test')\n",
    "  ax.legend()\n",
    "  ax.set_ylim([1.135, 1.18])\n",
    "  plt.ylabel('Root Mean Square Error')\n",
    "  plt.title('XGBoost RMSE')\n",
    "  fig = plt.gcf()\n",
    "\n",
    "  # Layer logs the plot\n",
    "  layer.log({\"Train vs Test - Model Overfit Check\": fig})\n",
    "\n",
    "  # clear all plots and figures from memory\n",
    "  plt.figure().clear()\n",
    "  plt.close()\n",
    "  plt.cla()\n",
    "  plt.clf()\n",
    "\n",
    "\n",
    "def check_model_performance(xgb_model: XGBRegressor, test_data_X, test_data_Y):\n",
    "  # PLOT 1: make predictions and show in a bar distribution plot\n",
    "  yhat = xgb_model.predict(test_data_X)\n",
    "  plt.hist(yhat)\n",
    "  fig1 = plt.gcf()\n",
    "\n",
    "  # Layer logs the plot\n",
    "  layer.log({\"Test Data Predicted Review Score Distribution\": fig1})\n",
    "\n",
    "  # clear all plots and figures from memory\n",
    "  plt.figure().clear()\n",
    "  plt.close()\n",
    "  plt.cla()\n",
    "  plt.clf()\n",
    "\n",
    "  # PLOT 2: distribution of actual review scores\n",
    "  plt.hist(test_data_Y)\n",
    "  fig2 = plt.gcf()\n",
    "\n",
    "  # Layer logs the plot\n",
    "  layer.log({\"Test Data Real Review Score Distribution\": fig2})\n",
    "\n",
    "  # clear all plots and figures from memory\n",
    "  plt.figure().clear()\n",
    "  plt.close()\n",
    "  plt.cla()\n",
    "  plt.clf()\n",
    "\n",
    "@pip_requirements(packages=[\"xgboost==0.90\"])\n",
    "@fabric(\"f-medium\")\n",
    "@model(\"review_score_predictor_model\",dependencies=[Dataset('training_data')])\n",
    "def train_final_model():\n",
    "  # The best parameter combination\n",
    "  param_dict = {\n",
    "    'colsample_bytree' : 1.0,\n",
    "    'learning_rate': 0.02,\n",
    "    'max_depth': 5,\n",
    "    'min_child_weight': 10,\n",
    "    'subsample' : 0.5\n",
    "  }\n",
    "\n",
    "  # Layer logs model description and model parameters\n",
    "  layer.log({\"Model Description\" : \"XGBRegressor with squared error objective function to predict review scores of orders based on their high level features.\"})\n",
    "  layer.log(param_dict)\n",
    "\n",
    "  training_data = layer.get_dataset(\"training_data\").to_pandas()\n",
    "  data_pair = data_split(training_data)\n",
    "\n",
    "  train_data_X = data_pair[0][0]\n",
    "  train_data_Y = data_pair[0][1]\n",
    "\n",
    "  test_data_X = data_pair[1][0]\n",
    "  test_data_Y = data_pair[1][1]\n",
    "\n",
    "  xgb_model_final = XGBRegressor(objective='reg:squarederror', n_estimators=200, colsample_bytree = param_dict['colsample_bytree'], learning_rate=param_dict['learning_rate'], max_depth=param_dict['max_depth'], min_child_weight=param_dict['min_child_weight'], subsample=param_dict['subsample'])\n",
    "  xgb_model_final.fit(train_data_X, train_data_Y,verbose=False)\n",
    "\n",
    "  model_overfit_check(train_data_X,train_data_Y, data_pair)\n",
    "  check_model_performance(xgb_model_final, test_data_X, test_data_Y)\n",
    "\n",
    "\n",
    "  return xgb_model_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Run your project on Layer in remote mode**\n",
    "\n",
    "Put all you functions into the layer.run(), then you are good to go! It will run these functions in its correct order regarding dependencies on Layer Infra remotely."
   ],
   "metadata": {
    "id": "RM99OQhn7mQM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0jOuxgR74VP"
   },
   "outputs": [],
   "source": [
    "## LAYER REMOTE MODE\n",
    "layer.run([load_order_table,\n",
    "           clean_order_table,\n",
    "           extract_features_order_table,\n",
    "           load_item_table,\n",
    "           clean_items_table,\n",
    "           extract_features_items_table,\n",
    "           load_reviews_table,\n",
    "           clean_reviews_table,\n",
    "           generate_training_data,\n",
    "           train_final_model])\n",
    "\n",
    "## LAYER LOCAL MODEL - Run your functions in order\n",
    "# load_order_table()\n",
    "# clean_order_table()\n",
    "# extract_features_order_table()\n",
    "# load_item_table()\n",
    "# clean_items_table()\n",
    "# extract_features_items_table()\n",
    "# load_reviews_table()\n",
    "# clean_reviews_table()\n",
    "# generate_training_data()\n",
    "# train_final_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Let's fetch our model and training dataset and make a prediction!**"
   ],
   "metadata": {
    "id": "ZxmN_fSv8KC6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import layer\n",
    "\n",
    "my_model = layer.get_model('layer/ecommerce_olist_order_review_score_prediction/models/review_score_predictor_model:2.1').get_train()\n",
    "\n",
    "df = layer.get_dataset('layer/ecommerce_olist_order_review_score_prediction/datasets/training_data:1.2').to_pandas()\n",
    "\n",
    "test_sample = df.drop(['review_score', 'order_id'], axis=1).sample()\n",
    "predicted_review_score = layer.get_model(\"review_score_predictor_model\").get_train().predict(test_sample)\n",
    "print(\"PREDICTED REVIEW SCORE [1-5]: \",predicted_review_score)\n"
   ],
   "metadata": {
    "id": "r3tGmiIthr1E",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "846b5f2a-6932-423d-950d-594ed1646476"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PREDICTED REVIEW SCORE [1-5]:  [4.376791]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Where to go from here?\n",
    "\n",
    "Now that you have created first Layer Project, you can:\n",
    "\n",
    "- Join our [Slack Community ](https://bit.ly/layercommunityslack)\n",
    "- Visit [Layer Examples Repo](https://github.com/layerai/examples) for more examples\n",
    "- Browse [Trending Layer Projects](https://layer.ai) on our mainpage\n",
    "- Check out [Layer Documentation](https://docs.app.layer.ai) to learn more"
   ],
   "metadata": {
    "id": "aFMK8uoC8FaM"
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ecommerce_order_review_score_prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}