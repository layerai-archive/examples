{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbl_5GrpV_Qd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Fine tuning T5 with Layer\n",
    "\n",
    "[![Open in Layer](https://development.layer.co/assets/badge.svg)](https://development.layer.co/layer/t5-fine-tuning-with-layer) [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/layerai/examples/blob/main/translation/T5_Fine_tuning_with_Layer.ipynb) [![Layer Examples Github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/layerai/examples/tree/main/translation)\n",
    "\n",
    "A T5 is an encoder-decoder model. It converts all NLP problems like language translation, summarization, text generation, question-answering, to a text-to-text task.\n",
    "\n",
    "We are going to fine tune a pretrained T5 Model from ðŸ¤— and train it to translate English to SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTOTXBFNKqhb"
   },
   "source": [
    "# Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkjmRTy_VQ72"
   },
   "outputs": [],
   "source": [
    "!pip install layer --upgrade -q\n",
    "!pip install sentencepiece -q\n",
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwlK1RxFKIWc"
   },
   "outputs": [],
   "source": [
    "from layer.decorators import dataset, model,resources, pip_requirements, fabric\n",
    "from layer import Model\n",
    "import layer\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZPKqz1aHjDc"
   },
   "source": [
    "# Getting Started with Layer\n",
    "\n",
    "Layer is an MLOps platform which advances ML pipelines with remote computation and tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4eVYhZ8K-Kf"
   },
   "source": [
    "## Login to Layer\n",
    "\n",
    "Let's login to Layer first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AW6owUH0VWe-"
   },
   "outputs": [],
   "source": [
    "layer.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vgj3JHo-JeNv"
   },
   "source": [
    "## Initialize Layer Project\n",
    "Now we are ready to init our project. Layer Project is basically an ML Repo hosted on Layer where you can store your datasets, models, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ezPdWXBXVlBP"
   },
   "outputs": [],
   "source": [
    "layer.init(\"t5-fine-tuning-with-layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xos7VwStJq_F"
   },
   "source": [
    "Your project is ready. Find your project here:\n",
    "\n",
    "https://app.layer.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63lSAPAiWeyz"
   },
   "source": [
    "# Dataset Generation\n",
    "Unlike language to language translation datasets, we can build custom English to SQL translation pairs programmatically with the help of some templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ablj8OlgWlC2",
    "outputId": "fff6ec69-bfd1-4548-e776-2d688a86ac0b"
   },
   "outputs": [],
   "source": [
    "templates = [\n",
    "              [\"[prop1] of [nns]\",\"SELECT [prop1] FROM [nns]\"],\n",
    "              [\"[agg] [prop1] for each [breakdown]\",\"SELECT [agg]([prop1]) , [breakdown] FROM [prop1] GROUP BY [breakdown]\"],\n",
    "              [\"[prop1] of [nns] by [breakdown]\",\"SELECT [prop1] , [breakdown] FROM [nns] GROUP BY [breakdown]\"],\n",
    "              [\"[prop1] of [nns] in [location] by [breakdown]\",\"SELECT [prop1] , [breakdown] FROM [nns] WHERE location = '[location]' GROUP BY [breakdown]\"],\n",
    "              [\"[nns] having [prop1] between [number1] and [number2]\",\"SELECT name FROM [nns] WHERE [prop1] > [number1] and [prop1] < [number2]\"],\n",
    "              [\"[prop] by [breakdown]\",\"SELECT name , [breakdown] FROM [prop] GROUP BY [breakdown]\"],\n",
    "              [\"[agg] of [prop1] of [nn]\",\"SELECT [agg]([prop1]) FROM [nn]\"],\n",
    "              [\"[prop1] of [nns] before [year]\",\"SELECT [prop1] FROM [nns] WHERE date < [year]\"],\n",
    "              [\"[prop1] of [nns] after [year] in [location]\",\"SELECT [prop1] FROM [nns] WHERE date > [year] AND location='[location]'\"],\n",
    "              [\"[nns] [verb] after [year] in [location]\",\"SELECT name FROM [nns] WHERE location = '[location]' AND date > [year]\"],\n",
    "              [\"[nns] having [prop1] between [number1] and [number2] by [breakdown]\",\"SELECT name , [breakdown] FROM [nns] WHERE [prop1] < [number1] AND [prop1] > [number2] GROUP BY [breakdown]\"],\n",
    "              [\"[nns] with a [prop1] of maximum [number1] by their [breakdown]\",\"SELECT name , [breakdown] FROM [nns] WHERE [prop1] <= [number1] GROUP BY [breakdown]\"],\n",
    "              [\"[prop1] and [prop2] of [nns] since [year]\",\"SELECT [prop1] , [prop2] FROM [nns] WHERE date > [year]\"],\n",
    "              [\"[nns] which have both [prop1] and [prop2]\",\"SELECT name FROM [nns] WHERE [prop1] IS true AND [prop2] IS true\"],\n",
    "              [\"Top [number1] [nns] by [prop1]\",\"SELECT name FROM [nns] ORDER BY [prop1] DESC LIMIT [number1]\"]\n",
    "]\n",
    "template = random.choice(templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbZ8S8m5YXNx"
   },
   "outputs": [],
   "source": [
    "objects = [\"countries\",\"wines\",\"wineries\",\"tasters\", \"provinces\",\"grapes\",\"cities\",\"bottles\",\"deliveries\"]\n",
    "object_single = [\"country\",\"wine\",\"winery\",\"taster\", \"province\",\"grape\",\"city\",\"bottle\", \"delivery\"]\n",
    "properties = [\"points\",\"price\",\"taste\",\"title\",\"texture\",\"age\",\"duration\",\"acidity\",\"flavor\",\"level\"]\n",
    "aggs = [[\"average\",\"avg\"], [\"total\",\"sum\"],[\"count\",\"count\"], [\"minimum\",\"min\"], [\"maximum\",\"max\"]]\n",
    "breakdowns = [\"quality\",\"price\",\"province\",\"country\",\"point\", \"variety\",\"flavor\",\"age\"]\n",
    "locations = [\"Italy\",\"US\",\"Portugal\",\"Spain\",\"Chile\",\"Turkey\",\"Canada\"]\n",
    "verbs = [\"produced\",\"bottled\"]\n",
    "\n",
    "regex = r\"\\[([a-z0-9]*)\\]\"\n",
    "number_of_samples = 2500\n",
    "\n",
    "@dataset(\"english_sql_translations\")\n",
    "def build_dataset():\n",
    "    rows = []\n",
    "    for index in range(0,number_of_samples):\n",
    "        template = random.choice(templates)\n",
    "        nl = template[0]\n",
    "        sql = template[1]\n",
    "\n",
    "        matches = re.finditer(regex, nl, re.MULTILINE)\n",
    "\n",
    "        for matchNum, match in enumerate(matches, start=1):\n",
    "            key = match.group()\n",
    "            prop = None\n",
    "            prop_sql = None\n",
    "            if key.startswith(\"[prop\"):\n",
    "                prop = random.choice(properties)\n",
    "                prop_sql = prop.replace(\" \",\"_\").lower()\n",
    "            if key in [\"[nns]\"]:\n",
    "                prop = random.choice(objects)\n",
    "                prop_sql = prop\n",
    "            if key in [\"[nn]\"]:\n",
    "                prop = random.choice(object_single)\n",
    "                prop_sql = prop.replace(\" \",\"_\").lower()\n",
    "            if key == \"[breakdown]\":\n",
    "                prop = random.choice(breakdowns)\n",
    "                prop_sql = prop.replace(\" \",\"_\").lower()\n",
    "            if key == \"[verb]\":\n",
    "                prop = random.choice(verbs)\n",
    "                prop_sql = prop.replace(\" \",\"_\").lower()\n",
    "            if key == \"[agg]\":\n",
    "                aggregation = random.choice(aggs)\n",
    "                prop = aggregation[0]\n",
    "                prop_sql = aggregation[1]\n",
    "            if key == \"[location]\":\n",
    "                prop = random.choice(locations)\n",
    "                prop_sql = prop\n",
    "            if key.startswith(\"[number\"):\n",
    "                prop = str(random.randint(1,1000))\n",
    "                prop_sql = prop\n",
    "            if key.startswith(\"[year\"):\n",
    "                prop = str(random.randint(1950,2022))\n",
    "                prop_sql = prop\n",
    "            \n",
    "\n",
    "            if prop is not None:\n",
    "                nl = nl.replace(key,prop)\n",
    "                sql = sql.replace(key,prop_sql)\n",
    "        \n",
    "        prefix = random.randint(1,20)\n",
    "        if prefix == 1:\n",
    "            nl = \"Show me \"+nl\n",
    "        elif prefix == 2:\n",
    "            nl = \"List \"+nl\n",
    "        elif prefix == 3:\n",
    "            nl = \"List of \"+nl\n",
    "        elif prefix == 4:\n",
    "            nl = \"Find \"+nl\n",
    "        rows.append([nl,sql])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"query\", \"sql\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BEjcCk_MJ95"
   },
   "source": [
    "## Register dataset to Layer\n",
    "\n",
    "In the above cell, we have used a special decorator called `@dataset` which tells Layer that our function creates dataset. Now we are going to pass this function to Layer to be run on Layer infra and register the built dataset under our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdhJXS-YLmtH"
   },
   "outputs": [],
   "source": [
    "layer.run([build_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhDn_iW5NLyt"
   },
   "source": [
    "# Fine Tune T5\n",
    "\n",
    "Our dataset is ready and registered to Layer. Now we are going to develop the fine tuning logic, decorate the function with `@model` and pass it to Layer so that it can be run on Layer infra and registered under our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUxkIObxoh8j"
   },
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "    import torch\n",
    "\n",
    "    model.train()\n",
    "    for _,data in enumerate(loader, 0):\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        step = (epoch * len(loader)) + _\n",
    "        layer.log({\"loss\": float(loss)}, step)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqA2YcZ8NdKS"
   },
   "source": [
    "Here we use 3 seperate Layer decorators:\n",
    "- [`@model`](https://docs.app.layer.ai/docs/sdk-library/model-decorator): Tells Layer that this function trains an ML model\n",
    "- [`@fabric`](https://docs.app.layer.ai/docs/sdk-library/fabric-decorator): Tells Layer the computation resources (cpu, gpu etc.) needed to train the model. Here is a list of the [available fabrics](https://docs.app.layer.ai/docs/reference/fabrics) you can use.\n",
    "- [`@pip_requirements`](https://docs.app.layer.ai/docs/sdk-library/pip-requirements-decorator): Tells the pypi libraries needed to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQYIvpAFoo2-"
   },
   "outputs": [],
   "source": [
    "@model(\"t5-tokenizer\")\n",
    "@fabric(\"f-medium\")\n",
    "@pip_requirements(packages=[\"torch\",\"transformers\",\"sentencepiece\"])\n",
    "def build_tokenizer():\n",
    "    from transformers import T5Tokenizer\n",
    "    # Load tokenizer from Hugging face\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "    return tokenizer\n",
    "\n",
    "@model(\"t5-english-to-sql\", dependencies=[Model(\"t5-tokenizer\")])\n",
    "@fabric(\"f-gpu-small\")\n",
    "@pip_requirements(packages=[\"torch\",\"transformers\",\"sentencepiece\"])\n",
    "def build_model():\n",
    "    from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "    from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "    import torch.nn.functional as F\n",
    "    from torch import cuda\n",
    "    import torch\n",
    "    \n",
    "    parameters={\n",
    "        \"BATCH_SIZE\":8,          \n",
    "        \"EPOCHS\":3,              \n",
    "        \"LEARNING_RATE\":2e-05,          \n",
    "        \"MAX_SOURCE_TEXT_LENGTH\":75,   \n",
    "        \"MAX_TARGET_TEXT_LENGTH\":75,\n",
    "        \"SEED\": 42\n",
    "    }\n",
    "\n",
    "    # Log parameters to Layer\n",
    "    layer.log(parameters)\n",
    "    \n",
    "    # Set seeds for reproducibility\n",
    "    torch.manual_seed(parameters[\"SEED\"])\n",
    "    np.random.seed(parameters[\"SEED\"])\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # Load tokenizer from Layer\n",
    "    tokenizer = layer.get_model(\"layer/t5-fine-tuning-with-layer/models/t5-tokenizer\").get_train()\n",
    "\n",
    "    # Load pretrained model from Hugging face\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "    device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    \n",
    "    def transform(row):\n",
    "        source_text = row.query\n",
    "        target_text = row.sql\n",
    "\n",
    "        source_text = ' '.join(source_text.split())\n",
    "        target_text = ' '.join(target_text.split())\n",
    "\n",
    "        source = tokenizer.batch_encode_plus([source_text], max_length= 75, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "        target = tokenizer.batch_encode_plus([target_text], max_length= 75, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long),\n",
    "            'source_mask': source_mask.to(dtype=torch.long),\n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    train_loader = layer.get_dataset(\"layer/t5-fine-tuning-with-layer/datasets/english_sql_translations\").to_pytorch(transform, batch_size=parameters[\"BATCH_SIZE\"], shuffle=True, num_workers=0)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=parameters[\"LEARNING_RATE\"])\n",
    "\n",
    "    for epoch in range(parameters[\"EPOCHS\"]):\n",
    "        train(epoch, tokenizer, model, device, train_loader, optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFWl17gYpOA-"
   },
   "outputs": [],
   "source": [
    "# # You can train your model locally by just calling the function to debug your code.\n",
    "# build_tokenizer()\n",
    "# build_model()\n",
    "\n",
    "# # Once you are ready, you can push your model training function to Layer to be trained.\n",
    "layer.run([build_tokenizer, build_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zvshXFgmHHb"
   },
   "source": [
    "## Where to go from here?\n",
    "\n",
    "Now that you have created first Layer Project, you can:\n",
    "\n",
    "- Join our [Slack Community ](https://bit.ly/layercommunityslack)\n",
    "- Visit [Layer Examples Repo](https://github.com/layerai/examples) for more examples\n",
    "- Browse [Trending Layer Projects](https://layer.ai) on our mainpage\n",
    "- Check out [Layer Documentation](https://docs.app.layer.ai) to learn more"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "T5 Fine tuning with Layer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
